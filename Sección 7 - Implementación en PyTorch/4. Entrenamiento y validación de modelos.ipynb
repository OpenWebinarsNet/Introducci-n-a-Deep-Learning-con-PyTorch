{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQyQPxls5WZb7U9VMKWc2Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Ahora vamos a desarrollar una función para poder entrenar la red, ya que hay que hacer el forward tanto para entrenamiento como para test vamos a utilizar la misma función.\n","\n","Dicha función recibirá el modelo, el dataloader, el optimizador y si estamos entrenando o no. Debemos tener un bucle que itere sobre cada batch del dataloader, en cada iteración el dataloader devuelve una tupla que contiene los datos de entrada y la salida esperada.\n","\n","Para cada tupla debemos poner a cero los gradientes de los pesos (si estamos entrenando). Después convertimos la entrada y el objetivo a un objeto llamado Variable dentro de autograd, esto hace que el objeto sea capaz de almacenar los gradientes y propagarlos. Una vez hecho esto podemos realizar el forward sobre la red y obtendremos la predicción (score) y el valor que devuelve la función de pérdida (loss).\n","\n","Con el valor de la predicción calculamos la clase predicha simplemente eligiendo la que obtenga el mayor valor y después comprobamos cuantas han sido correctas.\n","\n","En el caso de estar entrenando devemos propagar los gradientes con el algoritmo backward y después utilizar el optimizador para actualizar los pesos de la red. La función que acabamos de describir sería la siguiente:"],"metadata":{"id":"93jzVyTr5t_W"}},{"cell_type":"code","source":["def evaluate(model, dataset_loader, optimizer, train=False):\n","    correct_cnt, ave_loss = 0, 0#Contador de aciertos y acumulador de la función de pérdida\n","    count = 0#Contador de muestras\n","    for batch_idx, (x, target) in enumerate(dataset_loader):\n","        count += len(x)#sumamos el tamaño de batch, esto es porque n_batches*tamaño_batch != n_muestras\n","        if train:\n","            optimizer.zero_grad()#iniciamos a 0 los valores de los gradiente\n","        x, target = Variable(x), Variable(target)#Convertimos el tensor a variable del modulo autograd\n","        score, loss = model(x, target)#realizamos el forward\n","        _, pred_label = torch.max(score.data, 1)#pasamos de one hot a número\n","        correct_cnt += (pred_label == target.data).sum()#calculamos el número de etiquetas correctas\n","        ave_loss += loss.data[0]#sumamos el resultado de la función de pérdida para mostrar después\n","        if train:\n","            loss.backward()#Calcula los gradientes y los propaga\n","            optimizer.step()#adaptamos los pesos con los gradientes propagados\n","    accuracy = correct_cnt/count#Calculamos la precisión total\n","    ave_loss /= count#Calculamos la pérdida media\n","    print ('==>>>loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))#Mostramos resultados"],"metadata":{"id":"UGf7YUIi514d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Entrenar nuestro modelo ahora es muy sencillo, por ejemplo, si queremos realizar el proceso de entrenamiento durante 10 epochs testeando en cada una simplemente ejecutamos el siguiente código:"],"metadata":{"id":"OrSkRDKU55Vu"}},{"cell_type":"code","source":["for epoch in range(10):\n","    print(\"Epoch: {}\".format(epoch))\n","    print(\"Train\")\n","    evaluate(model, train_loader, optimizer, train=True)\n","    print(\"Test\")\n","    evaluate(model, test_loader, optimizer, train=False)"],"metadata":{"id":"XEnXHtb0568b"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTp8tCKt5k2K"},"outputs":[],"source":[]}]}