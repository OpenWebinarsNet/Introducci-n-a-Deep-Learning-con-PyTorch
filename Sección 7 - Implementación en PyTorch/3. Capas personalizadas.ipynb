{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZKbfr0vZrKIsgdc8B+SYh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Una vez tenemos el dataset debemos decidir que topología de red neuronal vamos a utilizar, en nuestro caso vamos a optar por una muy simple con una sola capa oculta que contiene 256 neuronas. Para ello vamos a definir una clase que desciende de nn.Module. En el __init__ definiremos las capas que va a tener la red y la función de pérdida que vamos a utilizar. La primera capa será la capa oculta y recibirá el tensor de entrada con los datos que tienen un tamaño de 28x28, por tanto la capa tendrá un tamaño de entrada de 28*28 y un tamaño de salida de 256. La segunda capa será la capa de salida que recibirá los datos de la capa oculta con un tamaño de 256 y su salida será igual al número de clases, en nuestro caso 10 clases. La función de pérdida que hemos escogido para nuestra red es la entropía cruzada.\n","\n","Después debemos definir la función forward, es decir, las operaciones que se van a realizar desde la entrada de la red hasta su salida. Aquí vamos a definir las funciones que se aplicarán de forma sucesiva a la red, en nuestro caso es muy sencillo. Primero debemos transformar la entrada que será una matriz de 28x28 en un vector de 784, para ello utilizaremos la función view, equivalente a reshape en numpy (el -1 se hace para que la primera dimensión, el tamaño de batch no sea alterada). Después aplicamos un función de activación a la salida de la capa oculta, en este caso hemos optado por la función rectified linear unit. Por último aplicamos la función softmax al tensor devuelto por la capa de salida, el parámetro dim indica en que dimensión se efectuará la función. Por último calculamos la función de pérdida, no es una práctica recomendable añadir el cálculo de dicha función al método forward ya que en el caso de únicamente querer predecir sería un cálculo absurdo pero en este caso lo haremos para que nos sea más sencillo entrenar la red. El código de dicha clase será el siguiente:"],"metadata":{"id":"EkSiSipV91nJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPYRS1T77oaJ"},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(28*28, 256)#capa oculta\n","        self.fc2 = nn.Linear(256, 10)#capa de salida\n","        self.loss_criterion = nn.CrossEntropyLoss()#Función de pérdida\n","\n","    def forward(self, x, target):\n","        x = x.view(-1, 28*28)#transforma las imágenes de tamaño (n, 28, 28) a (n, 784)\n","        x = F.relu(self.fc1(x))#Función de activación relu en la salida de la capa oculta\n","        x = F.softmax(self.fc2(x), dim=1)#Función de activación softmax en la salida de la capa oculta\n","        loss = self.loss_criterion(x, target)#Calculo de la función de pérdida\n","        return x, loss"]},{"cell_type":"markdown","source":["Ahora que tenemos el modelo de red neuronal creamos un objeto de dicha clase y el optimizador que vamos a utilizar para ajustar los pesos de la red. En nuestro caso será el descenso por gradiente estocástico el optimizador que utilizaremos con un ratio de aprendizaje de 0.1 y un momento de 0.9."],"metadata":{"id":"uROaiJYJ96RY"}},{"cell_type":"code","source":["model = MLP()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"],"metadata":{"id":"ci2hlj8X9UR9"},"execution_count":null,"outputs":[]}]}